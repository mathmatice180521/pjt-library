import logging
import re
import time # [ì¶”ê°€] íŒŒì¼ëª… íƒ€ì„ìŠ¤íƒ¬í”„ìš©

# [ì¶”ê°€] íŒŒì¼ ì €ì¥ ë° API ì‘ë‹µ ê´€ë ¨
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile
from rest_framework.decorators import api_view, permission_classes

from django.conf import settings
from django.db import transaction
from django.db.models import Q
from rest_framework import status
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from rest_framework.views import APIView

from books.models import Book
from interactions.models import Bookmark

from .models import AIRecommendation, AIRecommendationItem, AIContent
from .serializers import AIRecommendationSerializer, AIRecommendRequestSerializer

# [ìˆ˜ì •] utils.pyì— ì •ì˜ëœ í•¨ìˆ˜ë“¤ì„ ëª¨ë‘ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. (ImportError í•´ê²°)
from .utils import (
    build_user_preference_text,
    cosine_similarity,
    ensure_book_embedding,
    extract_intent_json,
    gemini_embed_text,
    generate_reason_for_book,
    # [ì¶”ê°€] ë§Œí™” ìƒì„± í•¨ìˆ˜
    generate_comic_image_file,
    # [ì¶”ê°€] utilsë¡œ ì´ë™í•œ fallback í•¨ìˆ˜ë“¤
    extract_keywords_fallback,
    build_keyword_filter_q, 
)

logger = logging.getLogger(__name__)


# =========================================================
# [ìˆ˜ì • ì „] utils.pyë¡œ ì´ë™ëœ í•¨ìˆ˜ë“¤ ì£¼ì„ ì²˜ë¦¬
# ì´ìœ : utils.pyì—ì„œ ì„í¬íŠ¸í•˜ì—¬ ì‚¬ìš©í•˜ë¯€ë¡œ ì¤‘ë³µ ì •ì˜ ì œê±°
# =========================================================
# STOPWORDS = { ... }
# def extract_keywords_fallback(...): ...
# def build_keyword_filter_q(...): ...


def _pub_sort_value(bk) -> int:
    pd = getattr(bk, "pub_date", None)
    if not pd:
        return 0
    try:
        return int(pd.toordinal())
    except Exception:
        try:
            return int(pd.timestamp())
        except Exception:
            return 0


def pick_candidates_by_keyword_score(qs, keywords: list[str], *, base_limit: int = 300, final_limit: int = 20):
    """
    ì„ë² ë”©ì´ ì—†ê±°ë‚˜ embed ì‹¤íŒ¨ ì‹œ fallback ë­í‚¹
    """
    base = list(qs.order_by("-customer_review_rank", "-pub_date")[:base_limit])
    if not keywords:
        return base[:final_limit]

    def score_book(bk) -> int:
        cat = bk.category.name if getattr(bk, "category", None) else ""
        txt = f"{bk.title} {bk.author} {bk.publisher} {cat} {bk.description or ''}".lower()
        return sum(1 for kw in keywords if kw and kw.lower() in txt)

    base.sort(
        key=lambda bk: (
            score_book(bk),
            bk.customer_review_rank or 0,
            _pub_sort_value(bk),
        ),
        reverse=True,
    )
    return base[:final_limit]


# =========================================================
# [ì¶”ê°€] 4ì»· ë§Œí™” ìƒì„± View
# =========================================================
@api_view(['POST'])
@permission_classes([IsAuthenticated])
def generate_comic_view(request, book_id):
    """
    [Bì•ˆ] íŠ¹ì • ì±…(book_id)ì— ëŒ€í•œ 4ì»· ë§Œí™” ìƒì„± API
    - ì´ë¯¸ ìƒì„±ëœ ê²Œ ìˆìœ¼ë©´ DB URL ë°˜í™˜
    - ì—†ìœ¼ë©´ Imagen í˜¸ì¶œ -> media ì €ì¥ -> URL ë°˜í™˜
    """
    try:
        book = Book.objects.get(pk=book_id)
    except Book.DoesNotExist:
        return Response({"error": "ì±…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}, status=404)

    # 1. ì´ë¯¸ ìƒì„±ëœ ì»¨í…ì¸ ê°€ ìˆëŠ”ì§€ í™•ì¸
    ai_content, created = AIContent.objects.get_or_create(book=book)
    
    # ì´ë¯¸ URLì´ ì¡´ì¬í•˜ë©´ ìƒˆë¡œ ë§Œë“¤ì§€ ì•Šê³  ë°˜í™˜ (ë¹„ìš© ì ˆì•½)
    if ai_content.comic_image_url:
        return Response({
            "book_id": book.id,
            "comic_url": ai_content.comic_image_url,
            "message": "ì´ë¯¸ ìƒì„±ëœ ë§Œí™”ê°€ ìˆì–´ ë°˜í™˜í•©ë‹ˆë‹¤."
        })

    # 2. ì´ë¯¸ì§€ ìƒì„± (ì‹œê°„ ì†Œìš”ë¨)
    try:
        # ìš”ì•½ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì±… ì†Œê°œê¸€ ì‚¬ìš©
        summary = ai_content.summary_text or book.description or "ì¬ë¯¸ìˆëŠ” ì´ì•¼ê¸°"
        
        # utils.pyì˜ í•¨ìˆ˜ í˜¸ì¶œí•˜ì—¬ ì´ë¯¸ì§€ íŒŒì¼ ê°ì²´(ContentFile) íšë“
        image_file = generate_comic_image_file(book.title, summary)
        
        # 3. íŒŒì¼ ì €ì¥ (Media í´ë”)
        # íŒŒì¼ëª… ìƒì„±: comics/comic_{book_id}_{timestamp}.png
        file_name = f"comics/comic_{book.id}_{int(time.time())}.png"
        file_path = default_storage.save(file_name, image_file)
        
        # 4. URL ìƒì„± (ë¡œì»¬ì´ë©´ /media/..., S3ë©´ https://...)
        full_url = default_storage.url(file_path)
        
        # 5. DB ì—…ë°ì´íŠ¸
        ai_content.comic_image_url = full_url
        ai_content.save()

        return Response({
            "book_id": book.id,
            "comic_url": full_url,
            "message": "ìƒˆë¡œìš´ ë§Œí™”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
        })

    except Exception as e:
        # [ë””ë²„ê¹…] êµ¬ì²´ì ì¸ ì—ëŸ¬ ë‚´ìš©ì„ ë¡œê·¸ì— ë‚¨ê¹€
        logger.exception("ë§Œí™” ìƒì„± ì‹¤íŒ¨")
        print(f"ğŸ”¥ [View Error] ë§Œí™” ìƒì„± ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return Response({"error": str(e)}, status=500)


class AIRecommendView(APIView):
    permission_classes = [IsAuthenticated]

    def get(self, request):
        """
        ë¡œê·¸ì¸ ìœ ì €ì˜ ì¶”ì²œ íˆìŠ¤í† ë¦¬ ëª©ë¡(ì—¬ëŸ¬ ê±´) ì¡°íšŒ
        """
        qs = (
            AIRecommendation.objects
            .filter(user=request.user)
            .prefetch_related("items__book")
            .order_by("-created_at")
        )

        total_count = qs.count()

        try:
            page = int(request.query_params.get("page", 1))
        except Exception:
            page = 1
        try:
            page_size = int(request.query_params.get("page_size", 10))
        except Exception:
            page_size = 10

        page = max(1, page)

        if page_size <= 0:
            recs = list(qs)
            data = AIRecommendationSerializer(recs, many=True).data
            return Response(
                {
                    "total_count": total_count,
                    "total_pages": 1 if total_count else 0,
                    "page": 1,
                    "page_size": total_count,
                    "results": data,
                },
                status=status.HTTP_200_OK,
            )

        page_size = max(1, page_size)
        total_pages = (total_count + page_size - 1) // page_size

        start = (page - 1) * page_size
        end = start + page_size

        recs = list(qs[start:end])
        data = AIRecommendationSerializer(recs, many=True).data

        return Response(
            {
                "total_count": total_count,
                "total_pages": total_pages,
                "page": page,
                "page_size": page_size,
                "results": data,
            },
            status=status.HTTP_200_OK,
        )


    def post(self, request):
        user = request.user

        # 0) ì…ë ¥ ê²€ì¦
        req = AIRecommendRequestSerializer(data=request.data)
        req.is_valid(raise_exception=True)
        v = req.validated_data

        prompt_text = (v.get("prompt") or "").strip()
        themes = v.get("themes") or []
        length = v.get("length")
        mood = v.get("mood")

        # 1) âœ… LLM ì˜ë„ ì¶”ì¶œ(JSON)
        try:
            intent = extract_intent_json(prompt_text)
        except Exception as e:
            logger.exception("intent ì¶”ì¶œ ì‹¤íŒ¨: %s", e)
            intent = {}

        core_topics = intent.get("core_topics") or []
        intent_line = intent.get("intent") or prompt_text
        request_type = intent.get("request_type")
        mood2 = intent.get("mood")
        avoid = intent.get("avoid") or []
        notes = intent.get("notes")

        if not mood and mood2:
            mood = mood2

        # intentê°€ ë¹ˆì•½í•˜ë©´ fallback í‚¤ì›Œë“œ ì‚¬ìš© (utils í•¨ìˆ˜ í˜¸ì¶œ)
        if not core_topics:
            core_topics = extract_keywords_fallback(prompt_text)

        # 2) ì‚¬ìš©ì ì„ í˜¸ í…ìŠ¤íŠ¸
        user_pref_text = build_user_preference_text(v)

        # 3) í›„ë³´ í’€ êµ¬ì„±
        candidates_qs = Book.objects.select_related("category", "ai_embedding")

        # (A) ìˆ˜í—˜ì„œ/ë¬¸ì œì§‘ ì œê±°
        EXCLUDE_CATEGORY = ["ìˆ˜í—˜", "ìê²©", "ê³µë¬´ì›", "ëŒ€í•™êµì¬", "í•™ìŠµ", "ë¬¸ì œì§‘"]
        EXCLUDE_TITLE = ["í•„ê¸°", "ì‹¤ê¸°", "ê¸°ì¶œ", "ëª¨ì˜ê³ ì‚¬", "Nì œ", "í•©ê²©", "ê¸°ì‚¬", "ì‚°ì—…ê¸°ì‚¬", "ìš´ì „ë©´í—ˆ"]
        for kw in EXCLUDE_CATEGORY:
            candidates_qs = candidates_qs.exclude(category__name__icontains=kw)
        for kw in EXCLUDE_TITLE:
            candidates_qs = candidates_qs.exclude(title__icontains=kw)

        # (B) ë¼ë…¸ë²¨/ì¥ë¥´ íŠ¸ë¦¬ê±°
        prompt_l = prompt_text.lower()
        manga_requested = any(k in prompt_l for k in ["ë§Œí™”", "ì›¹íˆ°", "ì½”ë¯¹", "comic"])

        LN_TRIGGERS = ["ë¼ì´íŠ¸ë…¸ë²¨", "ë¼ë…¸ë²¨", "ãƒ©ã‚¤ãƒˆãƒãƒ™ãƒ«", "í•˜ë ˜", "í•˜ë ˜ë¬¼", "ëŸ¬ë¸Œì½”ë¯¸ë””", "ì´ì„¸ê³„", "ì „ìƒ"]
        if any(k.lower() in prompt_l for k in LN_TRIGGERS) or any(t in core_topics for t in ["ë¼ì´íŠ¸ë…¸ë²¨", "ë¼ë…¸ë²¨", "í•˜ë ˜", "ì´ì„¸ê³„"]):
            ln_q = (
                Q(category__name__icontains="ë¼ì´íŠ¸") |
                Q(category__name__icontains="ë¼ë…¸") |
                Q(category__name__icontains="íŒíƒ€ì§€") |
                Q(category__name__icontains="ì†Œì„¤") |
                Q(category__name__icontains="ë¬¸í•™")
            )
            if manga_requested:
                ln_q |= Q(category__name__icontains="ë§Œí™”")
            candidates_qs = candidates_qs.filter(ln_q)

        # (C) ë¶ë§ˆí¬ ì œì™¸
        bookmarked_book_ids = Bookmark.objects.filter(user=user).values_list("book_id", flat=True)
        candidates_qs = candidates_qs.exclude(id__in=bookmarked_book_ids)

        # (D) core_topicsë¡œ ì•½í•˜ê²Œ í•„í„°ë§ (utils í•¨ìˆ˜ ì‚¬ìš©)
        if core_topics:
             filtered = candidates_qs.filter(build_keyword_filter_q(core_topics))
             if filtered.exists():
                 candidates_qs = filtered

        # 4) âœ… ì„ë² ë”© ì¿¼ë¦¬ êµ¬ì„±
        embed_query = f"""
[ì˜ë„] {intent_line}
[ì›ë¬¸] {prompt_text}
[í•µì‹¬í† í”½] {", ".join(core_topics[:6])}
""".strip()

        if request_type: embed_query += f"\n[ì›í•˜ëŠ”ë„ì›€] {request_type}"
        if mood: embed_query += f"\n[ë¶„ìœ„ê¸°] {mood}"
        if themes: embed_query += "\n[ì›í•˜ëŠ”ìš”ì†Œ] " + ", ".join(themes[:6])
        if length: embed_query += f"\n[ë¶„ëŸ‰] {length}"
        if notes: embed_query += f"\n[ì œì•½] {notes}"
        if avoid: embed_query += "\n[í”¼í•˜ê³ ì‹¶ìŒ] " + ", ".join([str(x) for x in avoid[:4]])

        # 5) ì‚¬ìš©ì ì¿¼ë¦¬ ì„ë² ë”©
        try:
            query_embedding = gemini_embed_text(embed_query)
        except Exception as e:
            logger.exception("Gemini embedContent ì‹¤íŒ¨: %s", e)
            query_embedding = None

        # 6) ì„ë² ë”© ê¸°ë°˜ ìŠ¤ì½”ì–´ë§
        scored = []
        lazy_limit = int(getattr(settings, "AI_EMBED_LAZY_LIMIT", 10))
        lazy_done = 0

        base_books = list(candidates_qs.order_by("-customer_review_rank", "-pub_date")[:250])

        if query_embedding:
            for bk in base_books:
                emb_obj = getattr(bk, "ai_embedding", None)

                if (not emb_obj) or (not emb_obj.embedding) or (not emb_obj.embedding_norm):
                    if lazy_done >= lazy_limit:
                        continue
                    try:
                        emb, emb_norm = ensure_book_embedding(bk, force=False)
                        lazy_done += 1
                    except Exception:
                        continue
                else:
                    emb = emb_obj.embedding
                    emb_norm = emb_obj.embedding_norm

                sim = cosine_similarity(query_embedding, emb, norm_b=emb_norm)
                if sim > -0.5:
                    scored.append((sim, bk))

        candidates: list[Book] = []
        if scored:
            scored.sort(
                key=lambda x: (
                    x[0],
                    x[1].customer_review_rank or 0,
                    _pub_sort_value(x[1]),
                ),
                reverse=True,
            )
            candidates = [bk for _, bk in scored[:20]]

        if not candidates:
            # ì„ë² ë”© ì‹¤íŒ¨ì‹œ fallback
            candidates = pick_candidates_by_keyword_score(candidates_qs, core_topics, base_limit=300, final_limit=20)

        top3 = candidates[:3]

        # 7) reason ìƒì„±
        picked = []
        for bk in top3:
            cat = bk.category.name if getattr(bk, "category", None) else ""
            txt = f"{bk.title} {bk.author} {bk.publisher} {cat} {bk.description or ''}".lower()
            match_topics = [kw for kw in core_topics if kw and kw.lower() in txt][:5]

            reason = generate_reason_for_book(
                user_pref_text=user_pref_text,
                user_keywords=core_topics,
                mood=mood,
                themes=themes,
                book=bk,
                match_keywords=match_topics,
            )

            picked.append({"book_pk": bk.id, "reason": reason})

        # 8) ì €ì¥
        with transaction.atomic():
            rec = AIRecommendation.objects.create(user=user)
            AIRecommendationItem.objects.bulk_create([
                AIRecommendationItem(
                    recommendation=rec,
                    book_id=item["book_pk"],
                    reason=item["reason"],
                )
                for item in picked
            ])

        rec = AIRecommendation.objects.filter(id=rec.id).prefetch_related("items__book").first()
        return Response(AIRecommendationSerializer(rec).data, status=status.HTTP_200_OK)